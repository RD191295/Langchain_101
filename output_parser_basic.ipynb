{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output parsers\n",
    "\n",
    "# Output Parsers are responsible for taking the output of an LLM and parsing into more structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# api for groq\n",
    "GROQ_API = os.getenv(\"GROQ_API\")\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat_groq_model = ChatGroq(\n",
    "    model='llama-3.1-70b-versatile',\n",
    "    api_key = GROQ_API,\n",
    "    temperature= 0.7,\n",
    "    max_retries=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comma Separated List OutputParser\n",
    "\n",
    "### Parse the output of an LLM call to a comma-separated list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "formated_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_prompt_template = PromptTemplate(\n",
    "    template = \"List the five {subject}. \\n {formated_instructions}\",\n",
    "    input_variables=['subject'],\n",
    "    partial_variables={'formated_instructions': formated_instructions},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = list_prompt_template | chat_groq_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokyo', 'Delhi', 'Shanghai', 'Mumbai', 'Sao Paulo']\n"
     ]
    }
   ],
   "source": [
    "output = chain.invoke({'subject': 'largest cities in the world'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date time OutputParser\n",
    "\n",
    "### This OutputParser can be used to parse LLM output into datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_datetime_parser = DatetimeOutputParser(Hformat = \"%Y-%m-%d\")\n",
    "formated_instructions = output_datetime_parser.get_format_instructions()\n",
    "\n",
    "date_time_prompt_template = PromptTemplate(\n",
    "    template=\"{question}\\n{formated_instructions}\",\n",
    "    input_variables=['question'],\n",
    "    partial_variables={'formated_instructions': formated_instructions},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time_chain = date_time_prompt_template | chat_groq_model | output_datetime_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = date_time_chain.invoke({'question': 'What is birth date of Narendra modi?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1950-09-17 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Json Ouput parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_parser = SimpleJsonOutputParser()\n",
    "json_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "  I want to Open a resturant for {cuisine} food. Suggest \n",
    "  a fancy name for this\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "  input_variables=['cuisine'],\n",
    "  template = prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajda\\AppData\\Local\\Temp\\ipykernel_1640\\288370439.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  json_chain = LLMChain(llm = chat_groq_model, prompt = prompt_template,\n"
     ]
    }
   ],
   "source": [
    "json_chain = LLMChain(llm = chat_groq_model, prompt = prompt_template,\n",
    "                 verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "  I want to Open a resturant for Italian food. Suggest \n",
      "  a fancy name for this\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res = json_chain.invoke(\"Italian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuisine': 'Italian', 'text': 'That sounds like a delicious venture. Here are some fancy name suggestions for your Italian restaurant:\\n\\n1. **Bella Vita**: Meaning \"Beautiful Life\" in Italian, this name evokes a sense of elegance and sophistication.\\n2. **Casa Toscana**: Inspired by the Tuscany region in Italy, this name conveys a sense of warmth and hospitality.\\n3. **Il Palazzo**: Meaning \"The Palace\" in Italian, this name suggests a grand and luxurious dining experience.\\n4. **Vino e Cucina**: Translating to \"Wine and Kitchen,\" this name highlights the importance of wine pairings and culinary expertise.\\n5. **La Dolce Vita**: Meaning \"The Sweet Life\" in Italian, this name captures the essence of Italian cuisine and culture.\\n6. **Ristorante Firenze**: Named after the city of Florence, this name exudes refinement and culture.\\n7. **Caffè Milano**: Inspired by the fashion capital of Italy, this name combines the idea of a casual café with the elegance of Milan.\\n8. **Tavola Italiana**: Meaning \"Italian Table\" in Italian, this name emphasizes the importance of family, friends, and food.\\n9. **Il Bistro Italiano**: This name combines the French term \"bistro\" with Italian flair, suggesting a cozy and intimate dining experience.\\n10. **Giovanni\\'s**: Named after a classic Italian name, this name adds a personal touch and suggests a warm, inviting atmosphere.\\n\\nChoose the one that resonates with your vision and style, and buon appetito!'}\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JsonOutputParser (With Pydantic)\n",
    "\n",
    "### This output parser allows users to specify an arbitrary JSON schema and query LLMs for outputs that conform to that schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic class for the case summary\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel,Field\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaseSummary(BaseModel):\n",
    "    issue: list[str] = Field(description=\"Issue of support case\")\n",
    "    root_cause: list[str] = Field(description=\"Root Cause of support case\")\n",
    "    resolution: list[str] = Field(description=\"Resolution of support case\")\n",
    "    case_status: list[str] = Field(description=\"Case Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_parser = JsonOutputParser(pydantic_object = CaseSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert support engineer.\n",
    "    Help me to summarize the below case into issues, root cause , resolution and case status.\n",
    "    Each bullet point should be a full sentance.\n",
    "    Avoid showing an personal information.\n",
    "    If you do not know answer , say \"I do not know\"\n",
    "    {formated_instructions}\n",
    "    case: \\n{case}\n",
    "    \"\"\",\n",
    "    description=\"Summarize the support case\",\n",
    "    input_variables=['case'],\n",
    "    partial_variables={'formated_instructions': json_parser.get_format_instructions()}  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_chain = LLMChain(llm = chat_groq_model, prompt = prompt_template,\n",
    "                 verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an expert support engineer.\n",
      "    Help me to summarize the below case into issues, root cause , resolution and case status.\n",
      "    Each bullet point should be a full sentance.\n",
      "    Avoid showing an personal information.\n",
      "    If you do not know answer , say \"I do not know\"\n",
      "    The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"issue\": {\"title\": \"Issue\", \"description\": \"Issue of support case\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"root_cause\": {\"title\": \"Root Cause\", \"description\": \"Root Cause of support case\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"resolution\": {\"title\": \"Resolution\", \"description\": \"Resolution of support case\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"case_status\": {\"title\": \"Case Status\", \"description\": \"Case Status\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"issue\", \"root_cause\", \"resolution\", \"case_status\"]}\n",
      "```\n",
      "    case: \n",
      "\n",
      "                  case: \n",
      "Customer is facing issue with the product. The product is not working as expected. The customer is not able to login to system. \n",
      "                  The customer is not able to access the data.  Root cause is system os is got corrupted. The resolution is to reinstall the os. cASE status is closed.\n",
      "                  \n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = json_chain.invoke(\"\"\"\n",
    "                  case: \\nCustomer is facing issue with the product. The product is not working as expected. The customer is not able to login to system. \n",
    "                  The customer is not able to access the data.  Root cause is system os is got corrupted. The resolution is to reinstall the os. cASE status is closed.\n",
    "                  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a JSON instance that summarizes the case:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"issue\": [\"The product is not working as expected.\", \"The customer is not able to login to system.\", \"The customer is not able to access the data.\"],\n",
      "  \"root_cause\": [\"The system os is corrupted.\"],\n",
      "  \"resolution\": [\"Reinstall the os.\"],\n",
      "  \"case_status\": [\"The case is closed.\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(output[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joker(BaseModel):\n",
    "    setup: str = Field(description=\"question to setup joke\")\n",
    "    punchline: str = Field(description=\"Answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_json_pareser  = JsonOutputParser(pydantic_object = Joker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_query = \"tell me a joke on cat\"\n",
    "\n",
    "joke_prompt_template = PromptTemplate(\n",
    "    template = \"Answer the user query. \\n {formated_instructions} \\n {query} \\n\",\n",
    "    description = \"Tell me a joke\",\n",
    "    input_variables = ['query'],\n",
    "    partial_variables = {'formated_instructions': joke_json_pareser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_chain = LLMChain(llm = chat_groq_model, prompt = joke_prompt_template,verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the user query. \n",
      " The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to setup joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"Answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "``` \n",
      " tell me a joke  \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = joke_chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"setup\": \"Why couldn't the bicycle stand up by itself?\",\n",
      "  \"punchline\": \"Because it was two-tired.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(output[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define chain with ne way (LCEL)\n",
    "\n",
    "joke_chain_v1 = joke_prompt_template | chat_groq_model | joke_json_pareser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the user query. \n",
      " The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to setup joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"Answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "``` \n",
      " tell me a joke on cat \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "joke_query = \"tell me a joke on cat\"\n",
    "output = joke_chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"setup\": \"Why did the cat join a band?\",\n",
      "  \"punchline\": \"Because it wanted to be the purr-cussionist.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(output[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSonOuputParser (Without Pydantic)\n",
    "\n",
    "### You can also use this without Pydantic. This will prompt it return JSON, but doesn't provide specific about what the schema should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_parser = JsonOutputParser()\n",
    "\n",
    "formated_instructions = json_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_query_v1 = \"\"\" Tell me a joke\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate (\n",
    "    template= \"Answer the user query as best as possible \\n{formated_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables= {'formated_instructions' : formated_instructions} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joke': \"Why don't scientists trust atoms? Because they make up everything.\", 'category': 'chemistry', 'type': 'pun'}\n"
     ]
    }
   ],
   "source": [
    "chain_joke = prompt_template | chat_groq_model | json_parser\n",
    "output = chain_joke.invoke({'query': joke_query_v1})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StructuredOutputParser\n",
    "\n",
    "### It is used when you want to parse an LLM’s response into a structured format like a dict, or JSON.\n",
    "### The StructuredOutputParser allows you to define a custom schema that matches the expected structure of the LLM’s response.\n",
    "### You would use the StructuredOutputParser when:\n",
    "\n",
    "#### <li>The LLM’s response contains multiple fields/values you want to extract</li>\n",
    "#### <li>The fields have predictable names you can define in a schema</li>\n",
    "#### <li>You want the output parsed into a dict rather than raw text</li>\n",
    "#### <li>The built-in parsers don’t handle the structure you need</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schema = [\n",
    "    ResponseSchema(name = 'answer',description='answer to user question'),\n",
    "    ResponseSchema(name = 'fact' , description=\"an intresting fact about answer to user question\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_structured_parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
    "formated_instructions = output_structured_parser.get_format_instructions() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"answer the user question as best as possible \\n{formated_instructions} \\n{query}\",\n",
    "    input_variables=['query'],\n",
    "    partial_variables= {'formated_instructions' : formated_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | chat_groq_model | output_structured_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = prompt_template.format_prompt(query=\"what's the capital of Manitoba?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({'query' : _input.to_messages()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winnipeg\n",
      "Winnipeg is not only the capital of Manitoba but also the largest city in the province, known for its vibrant arts and culture scene, as well as its historic significance as a major transportation hub in Canada.\n"
     ]
    }
   ],
   "source": [
    "print(output['answer'])\n",
    "print(output['fact'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enum parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.enum import EnumOutputParser\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Color(Enum):\n",
    "    RED = 'red'\n",
    "    GREEN = 'green'\n",
    "    BLUE = 'blue'\n",
    "    BROWN = 'brown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = EnumOutputParser(enum = Color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"What color eyes does this person have ? You should give only color and no other thing.\n",
    "    Answer should in one word only that is color in small letter.\n",
    "     \n",
    "    > Person : {person}\n",
    "\n",
    "    Instructions : {instructions}   \n",
    "     \"\"\"\n",
    ").partial(instructions = parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | chat_groq_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({'person' : 'Sardar Patel'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Color.BROWN: 'brown'>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

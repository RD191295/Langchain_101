{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output parsers\n",
    "\n",
    "# Output Parsers are responsible for taking the output of an LLM and parsing into more structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# api for groq\n",
    "GROQ_API = os.getenv(\"GROQ_API\")\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat_groq_model = ChatGroq(\n",
    "    model='llama-3.1-70b-versatile',\n",
    "    api_key = GROQ_API,\n",
    "    temperature= 0.7,\n",
    "    max_retries=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comma Separated List OutputParser\n",
    "\n",
    "### Parse the output of an LLM call to a comma-separated list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "formated_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_prompt_template = PromptTemplate(\n",
    "    template = \"List the five {subject}. \\n {formated_instructions}\",\n",
    "    input_variables=['subject'],\n",
    "    partial_variables={'formated_instructions': formated_instructions},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = list_prompt_template | chat_groq_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokyo', 'Delhi', 'Shanghai', 'Mumbai', 'Sao Paulo']\n"
     ]
    }
   ],
   "source": [
    "output = chain.invoke({'subject': 'largest cities in the world'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date time OutputParser\n",
    "\n",
    "### This OutputParser can be used to parse LLM output into datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_datetime_parser = DatetimeOutputParser(Hformat = \"%Y-%m-%d\")\n",
    "formated_instructions = output_datetime_parser.get_format_instructions()\n",
    "\n",
    "date_time_prompt_template = PromptTemplate(\n",
    "    template=\"{question}\\n{formated_instructions}\",\n",
    "    input_variables=['question'],\n",
    "    partial_variables={'formated_instructions': formated_instructions},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time_chain = date_time_prompt_template | chat_groq_model | output_datetime_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = date_time_chain.invoke({'question': 'What is birth date of Narendra modi?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1950-09-17 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Json Ouput parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_parser = SimpleJsonOutputParser()\n",
    "json_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "  I want to Open a resturant for {cuisine} food. Suggest \n",
    "  a fancy name for this\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "  input_variables=['cuisine'],\n",
    "  template = prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajda\\AppData\\Local\\Temp\\ipykernel_1640\\288370439.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  json_chain = LLMChain(llm = chat_groq_model, prompt = prompt_template,\n"
     ]
    }
   ],
   "source": [
    "json_chain = LLMChain(llm = chat_groq_model, prompt = prompt_template,\n",
    "                 verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "  I want to Open a resturant for Italian food. Suggest \n",
      "  a fancy name for this\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res = json_chain.invoke(\"Italian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuisine': 'Italian', 'text': 'That sounds like a delicious venture. Here are some fancy name suggestions for your Italian restaurant:\\n\\n1. **Bella Vita**: Meaning \"Beautiful Life\" in Italian, this name evokes a sense of elegance and sophistication.\\n2. **Casa Toscana**: Inspired by the Tuscany region in Italy, this name conveys a sense of warmth and hospitality.\\n3. **Il Palazzo**: Meaning \"The Palace\" in Italian, this name suggests a grand and luxurious dining experience.\\n4. **Vino e Cucina**: Translating to \"Wine and Kitchen,\" this name highlights the importance of wine pairings and culinary expertise.\\n5. **La Dolce Vita**: Meaning \"The Sweet Life\" in Italian, this name captures the essence of Italian cuisine and culture.\\n6. **Ristorante Firenze**: Named after the city of Florence, this name exudes refinement and culture.\\n7. **Caffè Milano**: Inspired by the fashion capital of Italy, this name combines the idea of a casual café with the elegance of Milan.\\n8. **Tavola Italiana**: Meaning \"Italian Table\" in Italian, this name emphasizes the importance of family, friends, and food.\\n9. **Il Bistro Italiano**: This name combines the French term \"bistro\" with Italian flair, suggesting a cozy and intimate dining experience.\\n10. **Giovanni\\'s**: Named after a classic Italian name, this name adds a personal touch and suggests a warm, inviting atmosphere.\\n\\nChoose the one that resonates with your vision and style, and buon appetito!'}\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JsonOutputParser (With Pydantic)\n",
    "\n",
    "### This output parser allows users to specify an arbitrary JSON schema and query LLMs for outputs that conform to that schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic class for the case summary\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel,Field\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaseSummary(BaseModel):\n",
    "    issue: list[str] = Field(description=\"Issue of support case\")\n",
    "    root_cause: list[str] = Field(description=\"Root Cause of support case\")\n",
    "    resolution: list[str] = Field(description=\"Resolution of support case\")\n",
    "    case_status: list[str] = Field(description=\"Case Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_parser = JsonOutputParser(pydantic_object = CaseSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are an expert support engineer.\n",
    "    Help me to summarize the below case into issues, root cause , resolution and case status.\n",
    "    Each bullet point should be a full sentance.\n",
    "    Avoid showing an personal information.\n",
    "    If you do not know answer , say \"I do not know\"\n",
    "    {formated_instructions}\n",
    "    case: \\n{case}\n",
    "    \"\"\",\n",
    "    description=\"Summarize the support case\",\n",
    "    input_variables=['case'],\n",
    "    partial_variables={'formated_instructions': json_parser.get_format_instructions()}  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_chain = LLMChain(llm = chat_groq_model, prompt = prompt_template,\n",
    "                 verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are an expert support engineer.\n",
      "    Help me to summarize the below case into issues, root cause , resolution and case status.\n",
      "    Each bullet point should be a full sentance.\n",
      "    Avoid showing an personal information.\n",
      "    If you do not know answer , say \"I do not know\"\n",
      "    The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"issue\": {\"title\": \"Issue\", \"description\": \"Issue of support case\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"root_cause\": {\"title\": \"Root Cause\", \"description\": \"Root Cause of support case\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"resolution\": {\"title\": \"Resolution\", \"description\": \"Resolution of support case\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"case_status\": {\"title\": \"Case Status\", \"description\": \"Case Status\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"issue\", \"root_cause\", \"resolution\", \"case_status\"]}\n",
      "```\n",
      "    case: \n",
      "\n",
      "                  case: \n",
      "Customer is facing issue with the product. The product is not working as expected. The customer is not able to login to system. \n",
      "                  The customer is not able to access the data.  Root cause is system os is got corrupted. The resolution is to reinstall the os. cASE status is closed.\n",
      "                  \n",
      "    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = json_chain.invoke(\"\"\"\n",
    "                  case: \\nCustomer is facing issue with the product. The product is not working as expected. The customer is not able to login to system. \n",
    "                  The customer is not able to access the data.  Root cause is system os is got corrupted. The resolution is to reinstall the os. cASE status is closed.\n",
    "                  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a JSON instance that summarizes the case:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"issue\": [\"The product is not working as expected.\", \"The customer is not able to login to system.\", \"The customer is not able to access the data.\"],\n",
      "  \"root_cause\": [\"The system os is corrupted.\"],\n",
      "  \"resolution\": [\"Reinstall the os.\"],\n",
      "  \"case_status\": [\"The case is closed.\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(output[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joker(BaseModel):\n",
    "    setup: str = Field(description=\"question to setup joke\")\n",
    "    punchline: str = Field(description=\"Answer to resolve the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_json_pareser  = JsonOutputParser(pydantic_object = Joker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_query = \"tell me a joke on cat\"\n",
    "\n",
    "joke_prompt_template = PromptTemplate(\n",
    "    template = \"Answer the user query. \\n {formated_instructions} \\n {query} \\n\",\n",
    "    description = \"Tell me a joke\",\n",
    "    input_variables = ['query'],\n",
    "    partial_variables = {'formated_instructions': joke_json_pareser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_chain = LLMChain(llm = chat_groq_model, prompt = joke_prompt_template,verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the user query. \n",
      " The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to setup joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"Answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "``` \n",
      " tell me a joke  \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = joke_chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"setup\": \"Why couldn't the bicycle stand up by itself?\",\n",
      "  \"punchline\": \"Because it was two-tired.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(output[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define chain with ne way (LCEL)\n",
    "\n",
    "joke_chain_v1 = joke_prompt_template | chat_groq_model | joke_json_pareser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the user query. \n",
      " The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to setup joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"Answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "``` \n",
      " tell me a joke on cat \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "joke_query = \"tell me a joke on cat\"\n",
    "output = joke_chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"setup\": \"Why did the cat join a band?\",\n",
      "  \"punchline\": \"Because it wanted to be the purr-cussionist.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(output[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSonOuputParser (Without Pydantic)\n",
    "\n",
    "### You can also use this without Pydantic. This will prompt it return JSON, but doesn't provide specific about what the schema should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_parser = JsonOutputParser()\n",
    "\n",
    "formated_instructions = json_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_query_v1 = \"\"\" Tell me a joke\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate (\n",
    "    template= \"Answer the user query as best as possible \\n{formated_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables= {'formated_instructions' : formated_instructions} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joke': \"Why don't scientists trust atoms? Because they make up everything.\", 'category': 'chemistry', 'type': 'pun'}\n"
     ]
    }
   ],
   "source": [
    "chain_joke = prompt_template | chat_groq_model | json_parser\n",
    "output = chain_joke.invoke({'query': joke_query_v1})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StructuredOutputParser\n",
    "\n",
    "### It is used when you want to parse an LLM’s response into a structured format like a dict, or JSON.\n",
    "### The StructuredOutputParser allows you to define a custom schema that matches the expected structure of the LLM’s response.\n",
    "### You would use the StructuredOutputParser when:\n",
    "\n",
    "#### <li>The LLM’s response contains multiple fields/values you want to extract</li>\n",
    "#### <li>The fields have predictable names you can define in a schema</li>\n",
    "#### <li>You want the output parsed into a dict rather than raw text</li>\n",
    "#### <li>The built-in parsers don’t handle the structure you need</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schema = [\n",
    "    ResponseSchema(name = 'answer',description='answer to user question'),\n",
    "    ResponseSchema(name = 'fact' , description=\"an intresting fact about answer to user question\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_structured_parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
    "formated_instructions = output_structured_parser.get_format_instructions() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"answer the user question as best as possible \\n{formated_instructions} \\n{query}\",\n",
    "    input_variables=['query'],\n",
    "    partial_variables= {'formated_instructions' : formated_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | chat_groq_model | output_structured_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = prompt_template.format_prompt(query=\"what's the capital of Manitoba?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({'query' : _input.to_messages()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winnipeg\n",
      "Winnipeg is not only the capital of Manitoba but also the largest city in the province, known for its vibrant arts and culture scene, as well as its historic significance as a major transportation hub in Canada.\n"
     ]
    }
   ],
   "source": [
    "print(output['answer'])\n",
    "print(output['fact'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enum parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.enum import EnumOutputParser\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Color(Enum):\n",
    "    RED = 'red'\n",
    "    GREEN = 'green'\n",
    "    BLUE = 'blue'\n",
    "    BROWN = 'brown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = EnumOutputParser(enum = Color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"What color eyes does this person have ? You should give only color and no other thing.\n",
    "    Answer should in one word only that is color in small letter.\n",
    "     \n",
    "    > Person : {person}\n",
    "\n",
    "    Instructions : {instructions}   \n",
    "     \"\"\"\n",
    ").partial(instructions = parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | chat_groq_model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke({'person' : 'Sardar Patel'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Color.BROWN: 'brown'>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "chat_mistral_model = ChatMistralAI(\n",
    "    model=\"mistral-large-latest\",\n",
    "    temperature=0.8,\n",
    "    api_key=os.environ['MISTRAL_API_KEY']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas DataFrame Parser\n",
    "\n",
    "### This output parser allows users to specify an arbitrary Pandas DataFrame and query LLMs for data in the form of a formatted dictionary that extracts data from the corresponding DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "from langchain.output_parsers import PandasDataFrameOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_parser_output(parser_output : dict[str, Any]) -> None:\n",
    "\n",
    "    for key in parser_output.keys():\n",
    "        parser_output[key] =  parser_output[key].to_dict()\n",
    "    return pprint.PrettyPrinter(width=4,compact=True).pprint(parser_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"num_legs\" : [1,2,3,4,5],\n",
    "        \"num_wings\" : [3,4,5,7,7],\n",
    "        \"num_specimen_seen\": [4,5,6,6,2]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_legs</th>\n",
       "      <th>num_wings</th>\n",
       "      <th>num_specimen_seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_legs  num_wings  num_specimen_seen\n",
       "0         1          3                  4\n",
       "1         2          4                  5\n",
       "2         3          5                  6\n",
       "3         4          7                  6\n",
       "4         5          7                  2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_dataframe = PandasDataFrameOutputParser(dataframe=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query = \"\"\"Retribe num_wings column data\"\"\"\n",
    "\n",
    "prompt_dataframe = PromptTemplate(\n",
    "    template= \"Answer the user Query \\n{formated_instructions} \\n {user_query}\",\n",
    "    input_variables=[\"user_query\"],\n",
    "    partial_variables= {'formated_instructions': parser_dataframe.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_pandas = prompt_dataframe | chat_groq_model | parser_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_wings': {0: 3,\n",
      "               1: 4,\n",
      "               2: 5,\n",
      "               3: 7,\n",
      "               4: 7}}\n"
     ]
    }
   ],
   "source": [
    "format_parser_output(chain_pandas.invoke({'user_query':df_query}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'num_legs': 1,\n",
      "       'num_specimen_seen': 4,\n",
      "       'num_wings': 3}}\n"
     ]
    }
   ],
   "source": [
    "df_query_1 = \"Retrieve the first row\"\n",
    "format_parser_output(chain_pandas.invoke({'user_query':df_query_1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 5.2}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_query_2 = \"Retrieve the mean of the num_wings column\"\n",
    "chain_pandas.invoke({'user_query':df_query_2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML parser\n",
    "\n",
    "### This output parser allows users to obtain results from LLM in the popular XML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import XMLOutputParser\n",
    "from langchain_cohere import ChatCohere\n",
    "import os\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_cohere_model = ChatCohere(\n",
    "    temperature=0.8,\n",
    "    api_key= os.environ['COHERE_API_KEY']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a shortened filmography of Tom Hanks, featuring some of his most notable films:\n",
      "\n",
      "<movie>Forrest Gump</movie>\n",
      "<movie>Saving Private Ryan</movie>\n",
      "<movie>Philadelphia</movie>\n",
      "<movie>Apollo 13</movie>\n",
      "<movie>Cast Away</movie>\n",
      "<movie>Big</movie>\n",
      "<movie>Toy Story</movie>\n",
      "<movie>Catch Me If You Can</movie>\n",
      "<movie>Captain Phillips</movie>\n",
      "<movie>The Green Mile</movie>\n",
      "<movie>Sully</movie>\n",
      "<movie>A Beautiful Day in the Neighborhood</movie>\n",
      "<movie>The Post</movie>\n",
      "\n",
      "This list includes award-winning dramas, iconic comedies, and animated classics, showcasing Hanks' versatility as an actor.\n"
     ]
    }
   ],
   "source": [
    "actor_query = \"Generate the shortend filmography for tom Hanks\"\n",
    "\n",
    "output = chat_cohere_model.invoke(f\"\"\" {actor_query} Please enclose the movies in <movie> </movie> tags \"\"\")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = XMLOutputParser()\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"{query}\\n{formated_instruction}\"\"\",\n",
    "    input_variables=['query'],\n",
    "    partial_variables={'formated_instruction': parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filmography': [{'film': [{'title': 'Forrest Gump'}, {'year': '1994'}]}, {'film': [{'title': 'Apollo 13'}, {'year': '1995'}]}, {'film': [{'title': 'Saving Private Ryan'}, {'year': '1998'}]}, {'film': [{'title': 'Cast Away'}, {'year': '2000'}]}, {'film': [{'title': 'Catch Me If You Can'}, {'year': '2002'}]}, {'film': [{'title': 'The Polar Express'}, {'year': '2004'}]}, {'film': [{'title': 'The Da Vinci Code'}, {'year': '2006'}]}, {'film': [{'title': 'Captain Phillips'}, {'year': '2013'}]}, {'film': [{'title': 'Bridge of Spies'}, {'year': '2015'}]}, {'film': [{'title': 'A Beautiful Day in the Neighborhood'}, {'year': '2019'}]}]}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt_template | chat_cohere_model | parser\n",
    "\n",
    "output = chain.invoke({'query' : actor_query})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add custom XML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_xml = XMLOutputParser(tags=['movies','Actor','films','director','name','genre'])\n",
    "\n",
    "prompt_xml = prompt_template = PromptTemplate(\n",
    "    template=\"\"\"{query}\\n{formated_instruction}\"\"\",\n",
    "    input_variables=['query'],\n",
    "    partial_variables={'formated_instruction': parser_xml.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_xml | chat_cohere_model | parser_xml\n",
    "\n",
    "actor_query = \"Generate the short filmography for tom Hanks \"\n",
    "\n",
    "output = chain.invoke({'query' : actor_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movies': [{'Actor': [{'films': [{'film': [{'genre': 'Romantic Comedy'}]}]}]}]}\n",
      "{'movies': [{'Actor': [{'films': [{'film': [{'genre': 'Comedy'}]}]}]}]}\n",
      "{'movies': [{'Actor': [{'films': [{'film': [{'genre': 'Drama'}]}]}]}]}\n",
      "{'movies': [{'Actor': [{'films': [{'film': [{'genre': 'War'}]}]}]}]}\n",
      "{'movies': [{'Actor': [{'films': [{'film': [{'genre': 'Adventure'}]}]}]}]}\n",
      "{'movies': [{'Actor': [{'director': [{'film': [{'genre': 'Musical Comedy'}]}]}]}]}\n",
      "{'movies': [{'Actor': [{'director': [{'film': [{'genre': 'Romantic Comedy'}]}]}]}]}\n"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"query\": actor_query}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAML parser\n",
    "\n",
    "### This output parser allows users to specify an arbitrary schema and query LLMs for outputs that conform to that schema, using YAML to format their response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel,Field\n",
    "from langchain.output_parsers import YamlOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup : str = Field(description='question to setup joke')\n",
    "    punchline : str = Field(description='answer to resolve joke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "user_query = 'Tell me a Joke'\n",
    "\n",
    "\n",
    "prompt_yaml = PromptTemplate(\n",
    "    template='Answer the user Query \\n{format_instruction}\\n{user_query}\\n',\n",
    "    input_variables=['user_query'],\n",
    "    partial_variables={'format_instruction': parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup='What did the hat say to the head?' punchline=\"You go on ahead, I'll catch up in a minute.\"\n"
     ]
    }
   ],
   "source": [
    "chain = prompt_yaml | chat_cohere_model | parser\n",
    "\n",
    "\n",
    "output = chain.invoke({'user_query':user_query})\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

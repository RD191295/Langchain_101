{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# api for groq & openai\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
<<<<<<< HEAD
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
=======
>>>>>>> 9fb96756a396a647ae90c27c63e0f9763ebcac69
    "\n",
    "chat_groq_model = ChatGroq(\n",
    "    model='llama-3.1-70b-versatile',\n",
    "    temperature= 0.7,\n",
    "    max_retries=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "  I want to Open a resturant for {cuisine} food. Suggest \n",
    "  a fancy one name for this\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "  input_variables=['cuisine'],\n",
    "  template = prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajda\\AppData\\Local\\Temp\\ipykernel_7652\\978229352.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm = chat_groq_model, prompt = prompt_template,\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm = chat_groq_model, prompt = prompt_template,\n",
    "                 verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajda\\AppData\\Local\\Temp\\ipykernel_7652\\2765289811.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  name = chain.run(cuisine = 'Italian')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "  I want to Open a resturant for Italian food. Suggest \n",
      "  a fancy one name for this\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "That's a great idea. Here are some fancy name suggestions for your Italian restaurant:\n",
      "\n",
      "1. **Bella Vita**: Meaning \"beautiful life\" in Italian, this name evokes a sense of elegance and sophistication.\n",
      "2. **Tavola Italiano**: \"Tavola\" means \"table\" in Italian, and this name suggests a warm and inviting dining experience.\n",
      "3. **Casa della Cucina**: This name means \"house of cooking\" in Italian, and it implies a cozy, welcoming atmosphere.\n",
      "4. **Ristorante Fiorentino**: This name references the city of Florence, known for its rich culinary history, and suggests a high-end dining experience.\n",
      "5. **Vino e Vita**: Meaning \"wine and life,\" this name combines two of Italy's greatest passions and implies a sophisticated, upscale atmosphere.\n",
      "6. **La Dolce Vita**: This name means \"the sweet life\" in Italian, and it suggests a restaurant that serves delicious, indulgent cuisine.\n",
      "7. **Cucina Classica**: This name means \"classic kitchen\" in Italian, and it implies a restaurant that serves traditional, high-quality Italian dishes.\n",
      "8. **Bistro Italiano**: This name combines the French word \"bistro\" with the Italian suffix \"-iano,\" suggesting a casual yet elegant dining experience.\n",
      "9. **Toscana**: This name references the Tuscany region of Italy, known for its beautiful landscapes and rich culinary heritage.\n",
      "10. **Il Palazzo**: Meaning \"the palace\" in Italian, this name suggests a grand, luxurious dining experience.\n",
      "\n",
      "Choose the one that resonates with your vision and style, and buon appetito!\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(cuisine = 'Italian')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "  I want to Open a resturant for Indian food. Suggest \n",
      "  a fancy one name for this\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Here are some fancy name suggestions for your Indian restaurant:\n",
      "\n",
      "1. **Tandoori Nights**: This name evokes the warmth and aroma of traditional Indian clay ovens, perfect for a fine dining experience.\n",
      "2. **Spice Route**: This name takes inspiration from the ancient trade routes that brought exotic spices to India, conveying a sense of luxury and sophistication.\n",
      "3. **Maharaja's Kitchen**: \"Maharaja\" means \"great king\" in Hindi, implying a regal and opulent dining experience.\n",
      "4. **Dastarkhwan**: This name means \"tablecloth\" in Urdu, but it also implies a lavish spread of delicious food, perfect for a high-end restaurant.\n",
      "5. **Jalsa**: This name means \"celebration\" or \"gathering\" in Hindi, suggesting a vibrant and festive atmosphere.\n",
      "6. **Saffron & Spice**: Saffron is a prized spice in Indian cuisine, and pairing it with \"spice\" implies a rich and aromatic experience.\n",
      "7. **Taj Mahal Spice**: The Taj Mahal is an iconic Indian landmark, and adding \"spice\" to the name implies a blend of traditional and modern flavors.\n",
      "8. **Bombay Bazaar**: This name captures the vibrant energy of Mumbai's street food scene, with a hint of luxury and sophistication.\n",
      "9. **Karma Kitchen**: \"Karma\" implies a sense of balance and harmony, suggesting a restaurant that serves food that's both delicious and nourishing.\n",
      "10. **Garam Masala**: This name incorporates a fundamental spice blend in Indian cuisine, implying a deep understanding of traditional flavors and techniques.\n",
      "\n",
      "Choose the one that resonates with your vision and atmosphere, and don't forget to check for availability of the desired web domain and social media handles!\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(cuisine = 'Indian')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "  I want to Open a resturant for CHINESE food. Suggest \n",
      "  a fancy one name for this\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Here are some fancy name suggestions for your Chinese restaurant:\n",
      "\n",
      "1. **Imperial Wok**: This name evokes the grandeur of Chinese imperial history and suggests a high-end dining experience.\n",
      "2. **Jade Emperor**: In Chinese culture, the Jade Emperor is a revered deity, and this name conveys a sense of luxury and refinement.\n",
      "3. **Dragon's Den**: This name has a dramatic, exotic flair and implies a bold, exciting culinary experience.\n",
      "4. **Silk Road Bistro**: The Silk Road was a historic trade route that connected China to the rest of the world, and this name suggests a fusion of flavors and cultures.\n",
      "5. **Golden Phoenix**: The phoenix is a mythical bird that symbolizes good fortune and prosperity in Chinese culture, making this name a fitting choice for a fancy restaurant.\n",
      "6. **Mandarin Palace**: This name references the Mandarin dialect and the imperial palaces of Chinese history, implying a regal, sophisticated atmosphere.\n",
      "7. **Chrysanthemum Court**: The chrysanthemum is a symbol of longevity and honor in Chinese culture, and this name suggests a refined, elegant dining experience.\n",
      "8. **Tao Garden**: \"Tao\" refers to the ancient Chinese philosophy of balance and harmony, and this name implies a serene, natural atmosphere.\n",
      "9. **Peking Pavilion**: This name references the historic city of Peking (now Beijing) and suggests a high-end, authentic Chinese dining experience.\n",
      "10. **Lucky Bamboo**: Bamboo is a symbol of good fortune in Chinese culture, and this name has a modern, upscale feel to it.\n",
      "\n",
      "Choose the one that resonates with your vision and style, and good luck with your restaurant venture!\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(cuisine = 'CHINESE')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBuffer Memory\n",
    "\n",
    "### this keeps in memory the conversation history to help contextualize the answer to the next user question. While this sounds very useful\n",
    "\n",
    "### one drawback is that it keeps all of history(upto the max limit of specific LLM) and for every questions passes the whole previous discussion (as tokens) to LLM API. \n",
    "\n",
    "### This can have significant cost impact as API costs are based on number of tokens processed and also the latency impact as conversation grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajda\\AppData\\Local\\Temp\\ipykernel_7652\\736173611.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = chat_groq_model, prompt = prompt_template, memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That sounds like a delicious venture. Here are some fancy name suggestions for your Italian restaurant:\n",
      "\n",
      "1. **Bella Vita**: Meaning \"beautiful life\" in Italian, this name evokes a sense of elegance and sophistication.\n",
      "2. **Tavola Italiano**: \"Tavola\" means \"table\" in Italian, and this name suggests a warm, inviting atmosphere.\n",
      "3. **Casa della Cucina**: Translating to \"house of cuisine,\" this name conveys a sense of refinement and expertise.\n",
      "4. **Vino e Vita**: Combining \"wine\" and \"life,\" this name implies a high-end dining experience with a focus on wine pairings.\n",
      "5. **Fiorella**: Meaning \"little flower\" in Italian, this name adds a touch of whimsy and romance to your restaurant.\n",
      "6. **La Dolce Vita**: Inspired by the famous Italian film, this name means \"the sweet life\" and promises a luxurious dining experience.\n",
      "7. **Caffè e Cucina**: This name combines the idea of a cozy café with a focus on cuisine, suggesting a warm and inviting atmosphere.\n",
      "8. **Il Palazzo**: Meaning \"the palace,\" this name implies a grand, upscale dining experience.\n",
      "9. **Bistro Italiano**: A classic name with a touch of elegance, this suggests a sophisticated, modern take on Italian cuisine.\n",
      "10. **Giovanni's**: Using a traditional Italian name, this suggests a family-run, authentic Italian restaurant with a touch of class.\n",
      "\n",
      "Choose the one that resonates with your vision and style, and buon appetito!\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(cuisine = 'Italian')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some fancy name suggestions for your Indian restaurant:\n",
      "\n",
      "1. **Tandoori Nights**: This name evokes the warm, inviting atmosphere of a traditional Indian tandoor oven, perfect for a fine dining experience.\n",
      "2. **Maharaja's Kitchen**: \"Maharaja\" means \"great king\" in Hindi, implying a regal and luxurious dining experience.\n",
      "3. **Saffron & Spice**: Saffron is a prized spice in Indian cuisine, and this name combines it with the warmth of spices to create a sophisticated and alluring title.\n",
      "4. **Dhaba Royale**: \"Dhaba\" is a Hindi word for a traditional roadside eatery, but adding \"Royale\" gives it a fancy twist, implying a high-end dining experience.\n",
      "5. **Bombay Bazaar**: This name captures the vibrant, exotic spirit of India's busiest city, Mumbai (formerly Bombay), and suggests a lively, upscale dining experience.\n",
      "6. **Karma Kitchens**: \"Karma\" is a Sanskrit word that implies a sense of destiny or fate, adding a touch of mystique to your restaurant's name.\n",
      "7. **The Spice Route**: This name references the historic trade routes that connected India to the rest of the world, implying a journey of discovery and exploration through cuisine.\n",
      "8. **Jaipur Jewels**: Jaipur is a city in India known for its rich cultural heritage and stunning jewelry, making this name a fitting choice for a fancy Indian restaurant.\n",
      "9. **The Tiffin Room**: \"Tiffin\" is a Hindi word for a light meal or snack, but this name adds a touch of elegance and sophistication, implying a refined dining experience.\n",
      "10. **Mumbai Masala**: This name combines the excitement of India's largest city with the warmth and richness of Indian spices, creating a catchy and upscale title for your restaurant.\n",
      "\n",
      "Choose the one that resonates with your vision and style, and good luck with your restaurant venture!\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(cuisine = 'Indian')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Italian\n",
      "AI: That sounds like a delicious venture. Here are some fancy name suggestions for your Italian restaurant:\n",
      "\n",
      "1. **Bella Vita**: Meaning \"beautiful life\" in Italian, this name evokes a sense of elegance and sophistication.\n",
      "2. **Tavola Italiano**: \"Tavola\" means \"table\" in Italian, and this name suggests a warm, inviting atmosphere.\n",
      "3. **Casa della Cucina**: Translating to \"house of cuisine,\" this name conveys a sense of refinement and expertise.\n",
      "4. **Vino e Vita**: Combining \"wine\" and \"life,\" this name implies a high-end dining experience with a focus on wine pairings.\n",
      "5. **Fiorella**: Meaning \"little flower\" in Italian, this name adds a touch of whimsy and romance to your restaurant.\n",
      "6. **La Dolce Vita**: Inspired by the famous Italian film, this name means \"the sweet life\" and promises a luxurious dining experience.\n",
      "7. **Caffè e Cucina**: This name combines the idea of a cozy café with a focus on cuisine, suggesting a warm and inviting atmosphere.\n",
      "8. **Il Palazzo**: Meaning \"the palace,\" this name implies a grand, upscale dining experience.\n",
      "9. **Bistro Italiano**: A classic name with a touch of elegance, this suggests a sophisticated, modern take on Italian cuisine.\n",
      "10. **Giovanni's**: Using a traditional Italian name, this suggests a family-run, authentic Italian restaurant with a touch of class.\n",
      "\n",
      "Choose the one that resonates with your vision and style, and buon appetito!\n",
      "Human: Indian\n",
      "AI: Here are some fancy name suggestions for your Indian restaurant:\n",
      "\n",
      "1. **Tandoori Nights**: This name evokes the warm, inviting atmosphere of a traditional Indian tandoor oven, perfect for a fine dining experience.\n",
      "2. **Maharaja's Kitchen**: \"Maharaja\" means \"great king\" in Hindi, implying a regal and luxurious dining experience.\n",
      "3. **Saffron & Spice**: Saffron is a prized spice in Indian cuisine, and this name combines it with the warmth of spices to create a sophisticated and alluring title.\n",
      "4. **Dhaba Royale**: \"Dhaba\" is a Hindi word for a traditional roadside eatery, but adding \"Royale\" gives it a fancy twist, implying a high-end dining experience.\n",
      "5. **Bombay Bazaar**: This name captures the vibrant, exotic spirit of India's busiest city, Mumbai (formerly Bombay), and suggests a lively, upscale dining experience.\n",
      "6. **Karma Kitchens**: \"Karma\" is a Sanskrit word that implies a sense of destiny or fate, adding a touch of mystique to your restaurant's name.\n",
      "7. **The Spice Route**: This name references the historic trade routes that connected India to the rest of the world, implying a journey of discovery and exploration through cuisine.\n",
      "8. **Jaipur Jewels**: Jaipur is a city in India known for its rich cultural heritage and stunning jewelry, making this name a fitting choice for a fancy Indian restaurant.\n",
      "9. **The Tiffin Room**: \"Tiffin\" is a Hindi word for a light meal or snack, but this name adds a touch of elegance and sophistication, implying a refined dining experience.\n",
      "10. **Mumbai Masala**: This name combines the excitement of India's largest city with the warmth and richness of Indian spices, creating a catchy and upscale title for your restaurant.\n",
      "\n",
      "Choose the one that resonates with your vision and style, and good luck with your restaurant venture!\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajda\\AppData\\Local\\Temp\\ipykernel_7652\\1333986198.py:1: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  ConversationChain = ConversationChain(llm = chat_groq_model, memory = memory)\n"
     ]
    }
   ],
   "source": [
    "ConversationChain = ConversationChain(llm = chat_groq_model, memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. I'm doing great, thanks for asking. I've been having a wonderful conversation with you about restaurant names, and I must say, it's been a real treat to explore the world of Italian and Indian cuisine with you. I've been trained on a vast amount of text data, and I'm always excited to share my knowledge and provide helpful suggestions. Speaking of which, I hope you found some inspiration for your restaurant's name from our previous conversation. If you're ready to move on, I'd be happy to chat with you about something new. What's on your mind today? Would you like to talk about food, travel, or perhaps something entirely different?\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Hello,How are you?\"\n",
    "response = ConversationChain.run(user_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_variables = memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Italian\n",
      "AI: That sounds like a delicious venture. Here are some fancy name suggestions for your Italian restaurant:\n",
      "\n",
      "1. **Bella Vita**: Meaning \"beautiful life\" in Italian, this name evokes a sense of elegance and sophistication.\n",
      "2. **Tavola Italiano**: \"Tavola\" means \"table\" in Italian, and this name suggests a warm, inviting atmosphere.\n",
      "3. **Casa della Cucina**: Translating to \"house of cuisine,\" this name conveys a sense of refinement and expertise.\n",
      "4. **Vino e Vita**: Combining \"wine\" and \"life,\" this name implies a high-end dining experience with a focus on wine pairings.\n",
      "5. **Fiorella**: Meaning \"little flower\" in Italian, this name adds a touch of whimsy and romance to your restaurant.\n",
      "6. **La Dolce Vita**: Inspired by the famous Italian film, this name means \"the sweet life\" and promises a luxurious dining experience.\n",
      "7. **Caffè e Cucina**: This name combines the idea of a cozy café with a focus on cuisine, suggesting a warm and inviting atmosphere.\n",
      "8. **Il Palazzo**: Meaning \"the palace,\" this name implies a grand, upscale dining experience.\n",
      "9. **Bistro Italiano**: A classic name with a touch of elegance, this suggests a sophisticated, modern take on Italian cuisine.\n",
      "10. **Giovanni's**: Using a traditional Italian name, this suggests a family-run, authentic Italian restaurant with a touch of class.\n",
      "\n",
      "Choose the one that resonates with your vision and style, and buon appetito!\n",
      "Human: Indian\n",
      "AI: Here are some fancy name suggestions for your Indian restaurant:\n",
      "\n",
      "1. **Tandoori Nights**: This name evokes the warm, inviting atmosphere of a traditional Indian tandoor oven, perfect for a fine dining experience.\n",
      "2. **Maharaja's Kitchen**: \"Maharaja\" means \"great king\" in Hindi, implying a regal and luxurious dining experience.\n",
      "3. **Saffron & Spice**: Saffron is a prized spice in Indian cuisine, and this name combines it with the warmth of spices to create a sophisticated and alluring title.\n",
      "4. **Dhaba Royale**: \"Dhaba\" is a Hindi word for a traditional roadside eatery, but adding \"Royale\" gives it a fancy twist, implying a high-end dining experience.\n",
      "5. **Bombay Bazaar**: This name captures the vibrant, exotic spirit of India's busiest city, Mumbai (formerly Bombay), and suggests a lively, upscale dining experience.\n",
      "6. **Karma Kitchens**: \"Karma\" is a Sanskrit word that implies a sense of destiny or fate, adding a touch of mystique to your restaurant's name.\n",
      "7. **The Spice Route**: This name references the historic trade routes that connected India to the rest of the world, implying a journey of discovery and exploration through cuisine.\n",
      "8. **Jaipur Jewels**: Jaipur is a city in India known for its rich cultural heritage and stunning jewelry, making this name a fitting choice for a fancy Indian restaurant.\n",
      "9. **The Tiffin Room**: \"Tiffin\" is a Hindi word for a light meal or snack, but this name adds a touch of elegance and sophistication, implying a refined dining experience.\n",
      "10. **Mumbai Masala**: This name combines the excitement of India's largest city with the warmth and richness of Indian spices, creating a catchy and upscale title for your restaurant.\n",
      "\n",
      "Choose the one that resonates with your vision and style, and good luck with your restaurant venture!\n",
      "Human: Hello,How are you?\n",
      "AI: Hello. I'm doing great, thanks for asking. I've been having a wonderful conversation with you about restaurant names, and I must say, it's been a real treat to explore the world of Italian and Indian cuisine with you. I've been trained on a vast amount of text data, and I'm always excited to share my knowledge and provide helpful suggestions. Speaking of which, I hope you found some inspiration for your restaurant's name from our previous conversation. If you're ready to move on, I'd be happy to chat with you about something new. What's on your mind today? Would you like to talk about food, travel, or perhaps something entirely different?\n"
     ]
    }
   ],
   "source": [
    "print(memory_variables['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Raj Dalsaniya. It's a pleasure to connect with a Data Scientist. I'm assuming you're interested in the intersection of data and artificial intelligence, which is a fascinating field. As a conversational AI, I'm trained on vast amounts of data, and I'm always eager to learn more about the latest developments in data science.\n",
      "\n",
      "I've been trained on a wide range of texts, from research papers to articles, and even books. My training data includes information on various topics, including machine learning, deep learning, natural language processing, and more. I'm not a replacement for a human data scientist, of course, but I can certainly provide insights and answer questions to the best of my ability.\n",
      "\n",
      "What specific areas of data science interest you the most, Raj? Are you working on any exciting projects currently? I'm all ears (or rather, all text) and would love to chat with you about your work. Perhaps we could discuss topics like predictive modeling, data visualization, or even the applications of data science in industries like healthcare or finance?\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Hello I am Raj Dalsaniya and Data Scientist\"\n",
    "response = ConversationChain.run(user_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Raj Dalsaniya, and you're a Data Scientist. I recall you introducing yourself earlier in our conversation.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"What is my name ?\"\n",
    "response = ConversationChain.run(user_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a Data Scientist, Raj. You mentioned that earlier in our conversation.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"What is my profession?\"\n",
    "response = ConversationChain.run(user_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConversationBufferWindowMemory\n",
    "\n",
    "\n",
    "### In Our conversations we usally do not need all the last 5-10 conversation history but definitely the last few. this type of memory helps define 'k' the number of last few consversation LLM should remember. It simply tells the LLM , remember the last few discussion (as per value K) and forget all of rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajda\\AppData\\Local\\Temp\\ipykernel_7652\\267246241.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  convo_buffer_memory = ConversationBufferWindowMemory(k = 1)\n"
     ]
    }
   ],
   "source": [
    "convo_buffer_memory = ConversationBufferWindowMemory(k = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_buffer_memory.save_context({\"inputs\":\"Hi\"},\n",
    "                                 {\"output\":\"What's up\"})\n",
    "\n",
    "convo_buffer_memory.save_context({\"inputs\":\"Not much, just hanging\"},\n",
    "                                 {\"output\":\"cool Buddy!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chat_groq_model, \n",
    "    memory = convo_buffer_memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: cool Buddy!'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_buffer_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi My name is Raj',\n",
       " 'history': 'Human: Not much, just hanging\\nAI: cool Buddy!',\n",
       " 'response': \"Nice to meet you Raj. I'm an AI, and I don't have a personal name, but I've been trained on a vast amount of text data, including books, articles, and conversations, which allows me to understand and respond to a wide range of topics and questions. I've been online since 2021, and my training data is constantly updated to keep my knowledge up to date. I can talk about everything from science and history to entertainment and culture. What brings you here today, Raj? Would you like to chat about something in particular or just see where the conversation takes us?\"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input = \"Hi My name is Raj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is 1+1',\n",
       " 'history': \"Human: Hi My name is Raj\\nAI: Nice to meet you Raj. I'm an AI, and I don't have a personal name, but I've been trained on a vast amount of text data, including books, articles, and conversations, which allows me to understand and respond to a wide range of topics and questions. I've been online since 2021, and my training data is constantly updated to keep my knowledge up to date. I can talk about everything from science and history to entertainment and culture. What brings you here today, Raj? Would you like to chat about something in particular or just see where the conversation takes us?\",\n",
       " 'response': \"That's a simple one, Raj. The answer to 1+1 is 2. This is a basic arithmetic operation that I've been trained on, and it's a fundamental concept in mathematics. In fact, I can perform calculations with numbers up to 15 decimal places, and I can even help with more complex mathematical operations like algebra and calculus if you need it. But I have to say, 1+1 is a great way to get our conversation started, and I'm glad we're beginning with something easy. Would you like to try something a bit more challenging, or is there something else on your mind that you'd like to talk about?\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input = \"what is 1+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name?',\n",
       " 'history': \"Human: what is 1+1\\nAI: That's a simple one, Raj. The answer to 1+1 is 2. This is a basic arithmetic operation that I've been trained on, and it's a fundamental concept in mathematics. In fact, I can perform calculations with numbers up to 15 decimal places, and I can even help with more complex mathematical operations like algebra and calculus if you need it. But I have to say, 1+1 is a great way to get our conversation started, and I'm glad we're beginning with something easy. Would you like to try something a bit more challenging, or is there something else on your mind that you'd like to talk about?\",\n",
       " 'response': 'You\\'re asking about your name. Unfortunately, I don\\'t have any information about your name, as we just started our conversation and you haven\\'t shared it with me yet. I did address you as \"Raj\" earlier, but that was just an error on my part - I sometimes try to use a name to make the conversation feel more personal, but I don\\'t actually know what your name is. If you\\'d like to share your name with me, I\\'d be happy to use it in our conversation and make things feel more friendly and personalized.'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input = \"what is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory\n",
    "\n",
    "### Instead of remembering the exact conversation, can we summarize the previous conversation context and hence help the LLM in answering the upcoming question? This is how Summary Memory helps. It keeps on summarizing the previous context and maintains it for use in next discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
=======
>>>>>>> 9fb96756a396a647ae90c27c63e0f9763ebcac69
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajda\\AppData\\Local\\Temp\\ipykernel_7652\\267246241.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  convo_buffer_memory = ConversationBufferWindowMemory(k = 1)\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "schedule_task = \"This is my tommorow schedule. There is a stadup meeting at 9:00 am to 9:30 am\\\n",
    "                You will need your powerpoint presentation to present with team at 10 am meeting regarding product design \\\n",
    "                after 10:30 am to 12:30 PM you will work on chatbot memory issue that currently facing \\\n",
    "                AFter 12:30 PM to 1:00 PM it's lunch time. after 1:00 pm to 3:00 pm work on langchain project to create our multilingual customer support chatbot \\\n",
    "                3:00 pm to 3:30 pm there is meeting with customer to understand customer requirement. \\\n",
    "                3:30 pm to 4:00 pm  it's tea break and refershment. At evening 4:00 PM tp 6:30 pm complete Devlopment Pending task \\\n",
    "                after 6:00 get ready for success party with team as last month our AI tool business you are leading is growm 10X\""
=======
    "convo_buffer_memory = ConversationBufferWindowMemory(k = 1)"
>>>>>>> 9fb96756a396a647ae90c27c63e0f9763ebcac69
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "memory = ConversationSummaryMemory(llm=chat_groq_model,\n",
    "                                   max_token_limit = 100)"
=======
    "convo_buffer_memory.save_context({\"inputs\":\"Hi\"},\n",
    "                                 {\"output\":\"What's up\"})\n",
    "\n",
    "convo_buffer_memory.save_context({\"inputs\":\"Not much, just hanging\"},\n",
    "                                 {\"output\":\"cool Buddy!\"})"
>>>>>>> 9fb96756a396a647ae90c27c63e0f9763ebcac69
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chat_groq_model, \n",
    "    memory = convo_buffer_memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule_task}\"})"
=======
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: cool Buddy!'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_buffer_memory.load_memory_variables({})"
>>>>>>> 9fb96756a396a647ae90c27c63e0f9763ebcac69
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "{'history': 'The human greets the AI with \"Hello\" and the AI responds with \"What\\'s up\". The human replies that they are not doing much, just hanging, and the AI responds with \"Cool\". The human then asks about the schedule for the day, and the AI outlines a busy schedule, including a standup meeting, a presentation with the team about product design, working on a chatbot memory issue, lunch, working on a langchain project for a multilingual customer support chatbot, a meeting with a customer, a tea break, completing pending development tasks, and ending the day with a success party to celebrate the 10X growth of the AI tool business they are leading.'}"
=======
       "{'input': 'Hi My name is Raj',\n",
       " 'history': 'Human: Not much, just hanging\\nAI: cool Buddy!',\n",
       " 'response': \"Nice to meet you Raj. I'm an AI, and I don't have a personal name, but I've been trained on a vast amount of text data, including books, articles, and conversations, which allows me to understand and respond to a wide range of topics and questions. I've been online since 2021, and my training data is constantly updated to keep my knowledge up to date. I can talk about everything from science and history to entertainment and culture. What brings you here today, Raj? Would you like to chat about something in particular or just see where the conversation takes us?\"}"
>>>>>>> 9fb96756a396a647ae90c27c63e0f9763ebcac69
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(memory = memory,\n",
    "                                 llm = chat_groq_model,\n",
    "                                 verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human greets the AI with \"Hello\" and the AI responds with \"What's up\". The human replies that they are not doing much, just hanging, and the AI responds with \"Cool\". The human then asks about the schedule for the day, and the AI outlines a busy schedule, including a standup meeting, a presentation with the team about product design, working on a chatbot memory issue, lunch, working on a langchain project for a multilingual customer support chatbot, a meeting with a customer, a tea break, completing pending development tasks, and ending the day with a success party to celebrate the 10X growth of the AI tool business they are leading.\n",
      "Human: which type of customer chatbot i will work tommrow?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'which type of customer chatbot i will work tommrow?',\n",
       " 'history': 'The human greets the AI with \"Hello\" and the AI responds with \"What\\'s up\". The human replies that they are not doing much, just hanging, and the AI responds with \"Cool\". The human then asks about the schedule for the day, and the AI outlines a busy schedule, including a standup meeting, a presentation with the team about product design, working on a chatbot memory issue, lunch, working on a langchain project for a multilingual customer support chatbot, a meeting with a customer, a tea break, completing pending development tasks, and ending the day with a success party to celebrate the 10X growth of the AI tool business they are leading.',\n",
       " 'response': \"You'll be working on a multilingual customer support chatbot, specifically using the Langchain project. This chatbot is designed to provide support to customers who speak different languages, so it's a pretty exciting and complex project. From what I recall, the goal is to integrate Langchain's capabilities into our existing chatbot framework to enable seamless communication with customers in their native languages. Does that sound about right to you?\"}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(\"which type of customer chatbot i will work tommrow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human greets the AI with \"Hello\" and the AI responds with \"What's up\". The human replies that they are not doing much, just hanging, and the AI responds with \"Cool\". The human then asks about the schedule for the day, and the AI outlines a busy schedule, including a standup meeting, a presentation with the team about product design, working on a chatbot memory issue, lunch, working on a langchain project for a multilingual customer support chatbot, a meeting with a customer, a tea break, completing pending development tasks, and ending the day with a success party to celebrate the 10X growth of the AI tool business they are leading. The human inquires about the specific customer chatbot they will be working on the next day, and the AI confirms it will be a multilingual customer support chatbot using the Langchain project, which aims to integrate Langchain's capabilities into the existing chatbot framework to enable communication with customers in their native languages.\n",
      "Human: what type of issue we are facing in chatbot?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what type of issue we are facing in chatbot?',\n",
       " 'history': 'The human greets the AI with \"Hello\" and the AI responds with \"What\\'s up\". The human replies that they are not doing much, just hanging, and the AI responds with \"Cool\". The human then asks about the schedule for the day, and the AI outlines a busy schedule, including a standup meeting, a presentation with the team about product design, working on a chatbot memory issue, lunch, working on a langchain project for a multilingual customer support chatbot, a meeting with a customer, a tea break, completing pending development tasks, and ending the day with a success party to celebrate the 10X growth of the AI tool business they are leading. The human inquires about the specific customer chatbot they will be working on the next day, and the AI confirms it will be a multilingual customer support chatbot using the Langchain project, which aims to integrate Langchain\\'s capabilities into the existing chatbot framework to enable communication with customers in their native languages.',\n",
       " 'response': \"We're currently facing a memory issue with the chatbot. Specifically, it's a problem with the chatbot's ability to retain context and recall previous conversations. The issue arises when the chatbot is handling a high volume of conversations simultaneously, causing it to sometimes fail to remember important details about the user's previous interactions. We're seeing error rates of around 5-7% where the chatbot is unable to recall the user's preferences or previous conversation history, which is affecting the overall user experience. I'll be working on resolving this issue today, exploring possible solutions such as optimizing the chatbot's memory allocation, implementing a more efficient caching mechanism, or even integrating a dedicated memory management system to improve the chatbot's ability to retain context.\"}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke('what type of issue we are facing in chatbot?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human greets the AI with \"Hello\" and the AI responds with \"What's up\". The human replies that they are not doing much, just hanging, and the AI responds with \"Cool\". The human then asks about the schedule for the day, and the AI outlines a busy schedule, including a standup meeting, a presentation with the team about product design, working on a chatbot memory issue, lunch, working on a langchain project for a multilingual customer support chatbot, a meeting with a customer, a tea break, completing pending development tasks, and ending the day with a success party to celebrate the 10X growth of the AI tool business they are leading. The human inquires about the specific customer chatbot they will be working on the next day, and the AI confirms it will be a multilingual customer support chatbot using the Langchain project, which aims to integrate Langchain's capabilities into the existing chatbot framework to enable communication with customers in their native languages. The human asks about the type of issue they are facing with the chatbot, and the AI explains that it's a memory issue, specifically a problem with the chatbot's ability to retain context and recall previous conversations, resulting in error rates of 5-7% where the chatbot fails to remember important user details, which the AI will work on resolving by exploring solutions such as optimizing memory allocation, implementing a more efficient caching mechanism, or integrating a dedicated memory management system.\n",
      "Human: what milestone we achieved last month?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what milestone we achieved last month?',\n",
       " 'history': 'The human greets the AI with \"Hello\" and the AI responds with \"What\\'s up\". The human replies that they are not doing much, just hanging, and the AI responds with \"Cool\". The human then asks about the schedule for the day, and the AI outlines a busy schedule, including a standup meeting, a presentation with the team about product design, working on a chatbot memory issue, lunch, working on a langchain project for a multilingual customer support chatbot, a meeting with a customer, a tea break, completing pending development tasks, and ending the day with a success party to celebrate the 10X growth of the AI tool business they are leading. The human inquires about the specific customer chatbot they will be working on the next day, and the AI confirms it will be a multilingual customer support chatbot using the Langchain project, which aims to integrate Langchain\\'s capabilities into the existing chatbot framework to enable communication with customers in their native languages. The human asks about the type of issue they are facing with the chatbot, and the AI explains that it\\'s a memory issue, specifically a problem with the chatbot\\'s ability to retain context and recall previous conversations, resulting in error rates of 5-7% where the chatbot fails to remember important user details, which the AI will work on resolving by exploring solutions such as optimizing memory allocation, implementing a more efficient caching mechanism, or integrating a dedicated memory management system.',\n",
       " 'response': \"We achieved a significant milestone last month - we reached 1 million conversations per day on our platform, which is a 20% increase from the previous month. This growth has been steady over the past quarter, with an average increase of 15% month-over-month. I recall that our team worked really hard to optimize the chatbot's performance, and we also launched a new feature that allowed users to access a knowledge base directly from the chat interface. This feature alone accounted for a 10% reduction in user queries, as users were able to find answers to their questions more quickly. The milestone we achieved last month has set us up well for our goal of reaching 5 million conversations per day by the end of the year, which is a key objective for our team.\"}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke('what milestone we achieved last month?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human greets the AI with \"Hello\" and the AI responds with \"What's up\". The human replies that they are not doing much, just hanging, and the AI responds with \"Cool\". The human then asks about the schedule for the day, and the AI outlines a busy schedule, including a standup meeting, a presentation with the team about product design, working on a chatbot memory issue, lunch, working on a langchain project for a multilingual customer support chatbot, a meeting with a customer, a tea break, completing pending development tasks, and ending the day with a success party to celebrate the 10X growth of the AI tool business they are leading. The human inquires about the specific customer chatbot they will be working on the next day, and the AI confirms it will be a multilingual customer support chatbot using the Langchain project, which aims to integrate Langchain's capabilities into the existing chatbot framework to enable communication with customers in their native languages. The human asks about the type of issue they are facing with the chatbot, and the AI explains that it's a memory issue, specifically a problem with the chatbot's ability to retain context and recall previous conversations, resulting in error rates of 5-7% where the chatbot fails to remember important user details, which the AI will work on resolving by exploring solutions such as optimizing memory allocation, implementing a more efficient caching mechanism, or integrating a dedicated memory management system. The human asks about the milestone achieved last month, and the AI reports that they reached 1 million conversations per day on their platform, a 20% increase from the previous month, driven by the team's efforts to optimize the chatbot's performance and the launch of a new feature that allowed users to access a knowledge base directly from the chat interface, which has set them up well for their goal of reaching 5 million conversations per day by the end of the year. The human asks when they will have free time tomorrow, and the AI estimates that they might have some free time after the tea break, around 3-4 pm, but notes that this is just a rough estimate and depends on how the day unfolds, with the possibility that the meeting with the customer or development tasks could run long and leave little free time.\n",
      "Human: when will i take tea break?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'when will i take tea break?',\n",
       " 'history': 'The human greets the AI with \"Hello\" and the AI responds with \"What\\'s up\". The human replies that they are not doing much, just hanging, and the AI responds with \"Cool\". The human then asks about the schedule for the day, and the AI outlines a busy schedule, including a standup meeting, a presentation with the team about product design, working on a chatbot memory issue, lunch, working on a langchain project for a multilingual customer support chatbot, a meeting with a customer, a tea break, completing pending development tasks, and ending the day with a success party to celebrate the 10X growth of the AI tool business they are leading. The human inquires about the specific customer chatbot they will be working on the next day, and the AI confirms it will be a multilingual customer support chatbot using the Langchain project, which aims to integrate Langchain\\'s capabilities into the existing chatbot framework to enable communication with customers in their native languages. The human asks about the type of issue they are facing with the chatbot, and the AI explains that it\\'s a memory issue, specifically a problem with the chatbot\\'s ability to retain context and recall previous conversations, resulting in error rates of 5-7% where the chatbot fails to remember important user details, which the AI will work on resolving by exploring solutions such as optimizing memory allocation, implementing a more efficient caching mechanism, or integrating a dedicated memory management system. The human asks about the milestone achieved last month, and the AI reports that they reached 1 million conversations per day on their platform, a 20% increase from the previous month, driven by the team\\'s efforts to optimize the chatbot\\'s performance and the launch of a new feature that allowed users to access a knowledge base directly from the chat interface, which has set them up well for their goal of reaching 5 million conversations per day by the end of the year. The human asks when they will have free time tomorrow, and the AI estimates that they might have some free time after the tea break, around 3-4 pm, but notes that this is just a rough estimate and depends on how the day unfolds, with the possibility that the meeting with the customer or development tasks could run long and leave little free time.',\n",
       " 'response': 'According to our schedule for the day, we have a tea break planned after we finish working on the Langchain project for the multilingual customer support chatbot and before we start completing the pending development tasks. So, it should be around 2:30-3:00 pm, but like I mentioned earlier, this is just a rough estimate and the actual time may vary depending on how the day unfolds.'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke('when will i take tea break?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory\n",
    "\n",
    "### Instead of “k” conversations being remembered in ConversationBufferWindowMemory, in this case we want to remember last set of discussion based on “max token limit”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajda\\AppData\\Local\\Temp\\ipykernel_7652\\3339641968.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory_CTBM = ConversationTokenBufferMemory(llm = chat_groq_model, max_token_limit= 25)\n"
     ]
    }
   ],
   "source": [
    "memory_CTBM = ConversationTokenBufferMemory(llm = chat_groq_model, max_token_limit= 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Data_Science_Study\\Course_Pracitse_Code\\GenerativeAi\\LangChain_Basic\\langenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Data_Science_Study\\Course_Pracitse_Code\\GenerativeAi\\LangChain_Basic\\langenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rajda\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "memory_CTBM.save_context({\"input\":\"AI is what?!\"},\n",
    "                    {\"output\":\"Amazing!\"})\n",
    "\n",
    "memory_CTBM.save_context({\"input\":\"Backprapogation is what?!\"},\n",
    "                    {\"output\":\"Beautiful!\"})\n",
    "\n",
    "memory_CTBM.save_context({\"input\":\"Chatbot is what?!\"},\n",
    "                    {\"output\":\"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Beautiful!\\nHuman: Chatbot is what?!\\nAI: Charming!'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_CTBM.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Max_token_limit is high \n",
    "memory_CTBM_1 = ConversationTokenBufferMemory(llm = chat_groq_model, max_token_limit= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_CTBM_1.save_context({\"input\":\"AI is what?!\"},\n",
    "                    {\"output\":\"Amazing!\"})\n",
    "\n",
    "memory_CTBM_1.save_context({\"input\":\"Backprapogation is what?!\"},\n",
    "                    {\"output\":\"Beautiful!\"})\n",
    "\n",
    "memory_CTBM_1.save_context({\"input\":\"Chatbot is what?!\"},\n",
    "                    {\"output\":\"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backprapogation is what?!\\nAI: Beautiful!\\nHuman: Chatbot is what?!\\nAI: Charming!'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_CTBM_1.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationSummaryBufferMemory\n",
    "\n",
    "### while ConversationSummaryMemory is good, we know that recent/last conversation has high correlation to upcoming query and a mix of summary of old conversation with buffer memory of last few conversation is good combination. And ConversationSummaryBufferMemory do this .\n",
    "\n",
    "### we can set Token Size which determine how much previous conversation to kept in buffer memory and how much need to summarise. \n",
    "\n",
    "### Higher the token size ---> more conversation to kept in buffer and less to summarise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajda\\AppData\\Local\\Temp\\ipykernel_23460\\3232312949.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(llm = chat_groq_model,\n",
      "d:\\Data_Science_Study\\Course_Pracitse_Code\\GenerativeAi\\LangChain_Basic\\langenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm = chat_groq_model,\n",
    "                                         max_token_limit=10)\n",
    "\n",
    "memory.save_context({\"input\":\"hi\"},\n",
    "                    {'output' : \"whats up\"})\n",
    "\n",
    "\n",
    "memory.save_context({\"input\":\"not much you\"},\n",
    "                    {'output' : \"not much\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human says \"hi\" and the AI responds with \"whats up\".\\nHuman: not much you\\nAI: not much'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also get the history as a list of messages.\n",
    "\n",
    "memory_1 = ConversationSummaryBufferMemory(llm = chat_groq_model,\n",
    "                                           max_token_limit=10,\n",
    "                                           return_messages=True)\n",
    "\n",
    "memory_1.save_context({\"input\":\"hi\"},\n",
    "                    {'output' : \"whats up\"})\n",
    "\n",
    "\n",
    "memory_1.save_context({\"input\":\"not much you\"},\n",
    "                    {'output' : \"not much\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human says \"hi\" and the AI responds with \"whats up\".', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='not much you', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='not much', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_1.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The human and the AI exchange brief and casual greetings, with the human saying \"not much you\" and the AI responding with \"not much\".'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = memory_1.chat_memory.messages\n",
    "previous_summary = \"\"\n",
    "\n",
    "memory_1.predict_new_summary(messages,previous_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using in Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajda\\AppData\\Local\\Temp\\ipykernel_23460\\3129015801.py:1: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "chain = ConversationChain(\n",
    "    llm = chat_groq_model,\n",
    "    memory = ConversationSummaryBufferMemory(llm = chat_groq_model,\n",
    "                                           max_token_limit=50),\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hello,what's up!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Hello,what's up!\",\n",
       " 'history': '',\n",
       " 'response': \"Not much, just processing a vast amount of text data I've been trained on. I was just going through a fascinating article about the history of artificial intelligence, and I came across the concept of the Turing Test, which was first proposed by Alan Turing in 1950. It's amazing to see how far we've come since then. I'm also currently running on a cloud-based infrastructure, utilizing a combination of natural language processing and machine learning algorithms to generate human-like responses. By the way, how's your day going so far?\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Hello,what's up!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI, and the AI responds by mentioning it's processing a vast amount of text data, including an article about the history of artificial intelligence, specifically the Turing Test, and explains its current infrastructure and capabilities, before asking the human about their day.\n",
      "Human: Just, Learning Langchain.. Currently learning about memory!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Just, Learning Langchain.. Currently learning about memory!',\n",
       " 'history': \"System: The human greets the AI, and the AI responds by mentioning it's processing a vast amount of text data, including an article about the history of artificial intelligence, specifically the Turing Test, and explains its current infrastructure and capabilities, before asking the human about their day.\",\n",
       " 'response': \"Memory is a fascinating topic, especially in the context of language models like myself. I'm currently running on a transformer-based architecture, which relies on self-attention mechanisms to weigh the importance of different input elements. This allows me to process and generate human-like text, but my memory is limited to the scope of the input I receive. I don't have a traditional memory like humans do, but rather a contextual understanding that's based on the patterns and relationships I've learned from the vast amount of text data I've been trained on.\\n\\nSpeaking of which, I was just processing an article about the history of artificial intelligence, and it mentioned the concept of memory in the context of the Turing Test. The test, proposed by Alan Turing in 1950, evaluates a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. One of the key aspects of the test is the machine's ability to learn and recall information, which is a fundamental aspect of human memory.\\n\\nI'm curious, what specifically are you learning about memory in the context of Langchain? Are you exploring the different types of memory, such as short-term, long-term, or working memory? Or perhaps you're looking into the applications of memory in natural language processing, such as language modeling or text generation? I'd be happy to help answer any questions you may have or provide more information on the topic.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Just, Learning Langchain.. Currently learning about memory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI, and the AI responds by mentioning it's processing a vast amount of text data, including an article about the history of artificial intelligence, specifically the Turing Test, and explains its current infrastructure and capabilities, before asking the human about their day. The human is learning about memory in the context of Langchain, and the AI discusses its own limited memory, which relies on self-attention mechanisms and contextual understanding based on patterns and relationships learned from its training data. The AI also mentions the concept of memory in the context of the Turing Test, and expresses curiosity about what specifically the human is learning about memory, offering to help answer questions or provide more information on the topic.\n",
      "Human: Do you know about Langchain Framework?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The Langchain Framework. Yes, I'm familiar with it. Langchain is an open-source framework designed to help developers build applications that leverage large language models like myself. It provides a set of tools and APIs that simplify the process of integrating language models into various projects, such as chatbots, virtual assistants, and other types of language-based applications.\\n\\nFrom what I've been trained on, Langchain allows developers to focus on building their applications without having to worry about the underlying complexities of natural language processing. It provides features like automated prompt engineering, response generation, and conversation management, making it easier to create conversational interfaces that are both intuitive and effective.\\n\\nI've also been trained on some of the documentation and tutorials related to Langchain, which cover topics like setting up the framework, configuring models, and optimizing performance. However, my knowledge is limited to my training data, and I may not have the most up-to-date information on the latest developments or features.\\n\\nIn the context of memory, I understand that Langchain provides some mechanisms for managing memory and storing information, but I'd love to learn more about what specifically you're exploring. Are you working on a project that involves using Langchain to build a conversational application, and if so, what kind of memory-related challenges are you trying to address? I'm here to help and provide any guidance or insights I can!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input = \"Do you know about Langchain Framework?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI, and the AI responds by mentioning it's processing a vast amount of text data, including an article about the history of artificial intelligence, specifically the Turing Test, and explains its current infrastructure and capabilities, before asking the human about their day. The human is learning about memory in the context of Langchain, and the AI discusses its own limited memory, which relies on self-attention mechanisms and contextual understanding based on patterns and relationships learned from its training data. The AI also mentions the concept of memory in the context of the Turing Test, and expresses curiosity about what specifically the human is learning about memory, offering to help answer questions or provide more information on the topic. The human asks about the Langchain Framework, and the AI explains that it is an open-source framework designed to help developers build applications that leverage large language models, providing tools and APIs that simplify the process of integrating language models into various projects, and also mentions that Langchain provides mechanisms for managing memory and storing information, while expressing interest in learning more about the human's specific exploration of memory in the context of Langchain, and offering to help with any project-related challenges.\n",
      "Human: Do you know about Langflow Framework?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Langflow Framework. While I\\'m familiar with the Langchain Framework, which I mentioned earlier, I don\\'t have specific information about a framework called \"Langflow.\" It\\'s possible that it\\'s a related or similar framework, or perhaps it\\'s a newer development that I haven\\'t been trained on. I\\'d love to learn more about it, though. Can you tell me more about the Langflow Framework and how it relates to your work with Langchain and memory? I\\'m curious to know if it\\'s a complementary tool, a competing framework, or something entirely different. If you\\'re willing to share, I\\'d be happy to listen and try to help if I can.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input = \"Do you know about Langflow Framework?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI, and the AI responds by mentioning it's processing a vast amount of text data, including an article about the history of artificial intelligence, specifically the Turing Test, and explains its current infrastructure and capabilities, before asking the human about their day. The human is learning about memory in the context of Langchain, and the AI discusses its own limited memory, which relies on self-attention mechanisms and contextual understanding based on patterns and relationships learned from its training data. The AI also mentions the concept of memory in the context of the Turing Test, and expresses curiosity about what specifically the human is learning about memory, offering to help answer questions or provide more information on the topic. The human asks about the Langchain Framework, and the AI explains that it is an open-source framework designed to help developers build applications that leverage large language models, providing tools and APIs that simplify the process of integrating language models into various projects, and also mentions that Langchain provides mechanisms for managing memory and storing information, while expressing interest in learning more about the human's specific exploration of memory in the context of Langchain, and offering to help with any project-related challenges. The human inquires about the Langflow Framework, and the AI, while familiar with Langchain, admits to not having specific information about Langflow, suggesting it might be a related, similar, or newer framework, and expresses a desire to learn more about it, asking the human to share details about Langflow and its relation to their work with Langchain and memory.\n",
      "Human: Haha nope, although a lot of people confuse it for that\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It seems like Langflow might be a distinct entity from Langchain, and I'm curious to learn more about the differences and similarities between the two. Can you tell me more about what Langflow is and how it relates to your work with Langchain and memory? Is it a separate framework, a tool, or perhaps a concept that builds upon or complements the capabilities of Langchain? I'm eager to understand the context and learn from your experience, as it might help me improve my knowledge and provide more accurate information in the future.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see here that the summary and the buffer are updated\n",
    "chain.predict(\n",
    "    input=\"Haha nope, although a lot of people confuse it for that\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableWithMessageHistory\n",
    "\n",
    "### Runnable that manages chat message history for another Runnable.\n",
    "\n",
    "### RunnableWithMessageHistory wraps another Runnable and manages the chat message history for it; it is responsible for reading and updating the chat message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're an assistant who's good at {ability}\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage,AIMessage\n",
    "from pydantic import BaseModel,Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"In memory implementation of chat message history.\"\"\"\n",
    "\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "\n",
    "    def add_messages(self, messages: List[BaseMessage]) -> None:\n",
    "        \"\"\"Add a list of messages to the store\"\"\"\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_by_session_id(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the message history\n",
    "message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_by_session_id,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key='history'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right-angled triangle. In simpler terms, cosine is a mathematical function that helps us calculate the length of one side of a right triangle when we know the length of another side and the angle between them. It is denoted by the abbreviation \"cos\" in mathematical equations.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 25, 'total_tokens': 104, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-8cb0a9c3-9d67-44c4-88ec-4e57734f0801-0' usage_metadata={'input_tokens': 25, 'output_tokens': 79, 'total_tokens': 104, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(message_history.invoke(  # noqa: T201\n",
    "    {\"ability\": \"math\", \"question\": \"What does cosine mean?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"foo\"}}\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'foo': InMemoryHistory(messages=[HumanMessage(content='What does cosine mean?', additional_kwargs={}, response_metadata={}), AIMessage(content='Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right-angled triangle. In simpler terms, cosine is a mathematical function that helps us calculate the length of one side of a right triangle when we know the length of another side and the angle between them. It is denoted by the abbreviation \"cos\" in mathematical equations.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 25, 'total_tokens': 104, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8cb0a9c3-9d67-44c4-88ec-4e57734f0801-0', usage_metadata={'input_tokens': 25, 'output_tokens': 79, 'total_tokens': 104, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})])}\n"
     ]
    }
   ],
   "source": [
    "print(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='1. Random Forest is an ensemble learning method that combines multiple decision trees to create a more accurate and stable predictive model.\\n2. Each decision tree in a Random Forest is trained on a random subset of the training data and a random subset of features, which helps to reduce overfitting and improve generalization.\\n3. Random Forest can be used for both classification and regression tasks, making it a versatile algorithm for a wide range of predictive modeling problems.\\n4. The final prediction of a Random Forest model is typically made by averaging the predictions of all the individual trees in the forest for regression tasks, or using a majority voting scheme for classification tasks.\\n5. Random Forest is known for its high performance, scalability, and ability to handle large datasets with high dimensionality. It is one of the most popular machine learning algorithms used in practice due to its robustness and effectiveness in handling complex problems.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 179, 'prompt_tokens': 31, 'total_tokens': 210, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-c14db994-3ece-4a8d-a798-1d88791c9c62-0' usage_metadata={'input_tokens': 31, 'output_tokens': 179, 'total_tokens': 210, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(message_history.invoke(  # noqa: T201\n",
    "    {\"ability\": \"data science\", \"question\": \"Please tell 5 point about Random Forest algorithm?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"jem\"}}\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'foo': InMemoryHistory(messages=[HumanMessage(content='What does cosine mean?', additional_kwargs={}, response_metadata={}), AIMessage(content='Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right-angled triangle. In simpler terms, cosine is a mathematical function that helps us calculate the length of one side of a right triangle when we know the length of another side and the angle between them. It is denoted by the abbreviation \"cos\" in mathematical equations.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 25, 'total_tokens': 104, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8cb0a9c3-9d67-44c4-88ec-4e57734f0801-0', usage_metadata={'input_tokens': 25, 'output_tokens': 79, 'total_tokens': 104, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]), 'jem': InMemoryHistory(messages=[HumanMessage(content='Please tell 5 point about Random Forest algorithm?', additional_kwargs={}, response_metadata={}), AIMessage(content='1. Random Forest is an ensemble learning method that combines multiple decision trees to create a more accurate and stable predictive model.\\n2. Each decision tree in a Random Forest is trained on a random subset of the training data and a random subset of features, which helps to reduce overfitting and improve generalization.\\n3. Random Forest can be used for both classification and regression tasks, making it a versatile algorithm for a wide range of predictive modeling problems.\\n4. The final prediction of a Random Forest model is typically made by averaging the predictions of all the individual trees in the forest for regression tasks, or using a majority voting scheme for classification tasks.\\n5. Random Forest is known for its high performance, scalability, and ability to handle large datasets with high dimensionality. It is one of the most popular machine learning algorithms used in practice due to its robustness and effectiveness in handling complex problems.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 179, 'prompt_tokens': 31, 'total_tokens': 210, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c14db994-3ece-4a8d-a798-1d88791c9c62-0', usage_metadata={'input_tokens': 31, 'output_tokens': 179, 'total_tokens': 210, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})])}\n"
     ]
    }
   ],
   "source": [
    "print(store)"
   ]
=======
    "conversation.invoke(input = \"Hi My name is Raj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is 1+1',\n",
       " 'history': \"Human: Hi My name is Raj\\nAI: Nice to meet you Raj. I'm an AI, and I don't have a personal name, but I've been trained on a vast amount of text data, including books, articles, and conversations, which allows me to understand and respond to a wide range of topics and questions. I've been online since 2021, and my training data is constantly updated to keep my knowledge up to date. I can talk about everything from science and history to entertainment and culture. What brings you here today, Raj? Would you like to chat about something in particular or just see where the conversation takes us?\",\n",
       " 'response': \"That's a simple one, Raj. The answer to 1+1 is 2. This is a basic arithmetic operation that I've been trained on, and it's a fundamental concept in mathematics. In fact, I can perform calculations with numbers up to 15 decimal places, and I can even help with more complex mathematical operations like algebra and calculus if you need it. But I have to say, 1+1 is a great way to get our conversation started, and I'm glad we're beginning with something easy. Would you like to try something a bit more challenging, or is there something else on your mind that you'd like to talk about?\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input = \"what is 1+1\")"
   ]
>>>>>>> 9fb96756a396a647ae90c27c63e0f9763ebcac69
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
